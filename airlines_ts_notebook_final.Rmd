---
title: "DSC551 Airlines Project"
output: html_notebook
---

# Data

```{r}
library(fpp2)
library(ggplot2)
library(forecast)
library(gridExtra)
library(seasonal)
library(tseries)
library(urca)
```

```{r}

air <- read.csv("US_Monthly_Air_Passengers00_19.csv")
```

```{r}
colnames(air)
```

# Nevada

```{r}
library(dplyr)
library(tidyr)
nv <- air %>%
  filter( ORIGIN_STATE_ABR == "NV" | DEST_STATE_ABR == "NV")

state <- air %>%
  filter(ORIGIN_STATE_ABR == "State Abbr" | DEST_STATE_ABR == "State Abbr")

```

#  Most Frequented Cities/Airports

```{r}
most_frequented_cities <- nv %>%
  # Reshape the data to stack ORIGIN and DEST into a single column
  pivot_longer(cols = c(ORIGIN, DEST), names_to = "Type", values_to = "City")
```

```{r}

# Get the list of unique cities from the ORIGIN column where ORIGIN_STATE_ABR is "NV"
nevada_cities <- nv %>%
  filter(ORIGIN_STATE_ABR == "NV") %>%
  distinct(ORIGIN) %>%
  pull(ORIGIN) # Extracts the column as a vector

most_frequented_cities <- nv %>%
  # Reshape the data to stack ORIGIN and DEST into a single column
  pivot_longer(cols = c(ORIGIN, DEST), names_to = "Type", values_to = "City") %>%
  # Group by the new City column
  group_by(City) %>%
  # Count the occurrences of each city
  summarise(Count = n(), .groups = 'drop') %>%
  # Filter to keep only the cities that are Nevada's ORIGIN cities
  filter(City %in% nevada_cities) %>%
  # Calculate the percentage for each city, rounded to two decimal places
  mutate(Percentage = round((Count / sum(Count)) * 100, 2)) %>%
  # Order by descending count to see the most frequented cities at the top
  arrange(desc(Count))

most_frequented_cities


```

```{r}
library(dplyr)
library(tidyr)

# Get the list of unique cities from the ORIGIN column where ORIGIN_STATE_ABR is "NV"
nevada_cities <- nv %>%
  filter(ORIGIN_STATE_ABR == "NV") %>%
  distinct(ORIGIN) %>%
  pull(ORIGIN) # Extracts the column as a vector

most_frequented_cities <- nv %>%
  # Reshape the data to stack ORIGIN and DEST into a single column
  pivot_longer(cols = c(ORIGIN, DEST), names_to = "Type", values_to = "City") %>%
  # Filter to keep only the cities that are Nevada's ORIGIN cities
  filter(City %in% nevada_cities) %>%
  # Group by the new City column and the Type
  group_by(City, Type) %>%
  # Count the occurrences of each city and type combination
  summarise(Count = n(), .groups = 'drop') %>%
  # Calculate the percentage for each type within each city
  mutate(Percentage = round((Count / sum(Count)) * 100, 2))

# Create a separate summary for total counts and percentages
total_summary <- most_frequented_cities %>%
  group_by(City) %>%
  summarise(Total_Count = sum(Count), .groups = 'drop') %>%
  mutate(Total_Percentage = round((Total_Count / sum(Total_Count)) * 100, 2))

# Now, create an overall summary table with separate counts and percentages for ORIGIN and DEST
# And also add the total counts and percentages
city_type_breakdown <- most_frequented_cities %>%
  # Pivot the table to have separate columns for ORIGIN and DEST counts and percentages
  pivot_wider(names_from = Type, values_from = c(Count, Percentage)) %>%
  left_join(total_summary, by = "City") %>%
  arrange(desc(Total_Count)) # Order by descending total count

city_type_breakdown

```

We find the Las Vegas and Reno are the most frequented airports in Nevada, making up a combined 93.88% of the total airtraffic in and out of Nevada.

# Most Frequented Destinations out of Nevada

```{r}
# Assuming nv is your data frame containing flight data
most_frequented_destinations <- nv %>%
  # Filter for flights originating from Nevada
  filter(ORIGIN_STATE_ABR == "NV") %>%
  # Group by both ORIGIN and DEST
  group_by(ORIGIN, DEST) %>%
  # Count the number of flights for each origin-destination pair
  summarise(Count = n(), .groups = 'drop') %>%
  # Order by descending count to find the most frequented destinations
  arrange(desc(Count))

most_frequented_destinations

```

## Frequency of flights with destination NV by origin

```{r}
library(dplyr)

# Assuming nv is your data frame containing flight data
most_frequented_origins <- nv %>%
  # Filter for flights arriving in Nevada
  filter(DEST_STATE_ABR == "NV") %>%
  # Group by both DEST and ORIGIN
  group_by(DEST, ORIGIN) %>%
  # Count the number of flights for each destination-origin pair
  summarise(Count = n(), .groups = 'drop') %>%
  # Order by descending count to find the most frequented origins
  arrange(desc(Count))

most_frequented_origins

```

Note that there is a pretty strong symmetry in the flow of flights across the near 20 year history. For almost every flight that go from Las Vegas to a destination we find a count for another flight coming to Las Vegas from said destination.

We could make two maps showing the flow of air traffic for the two and from.

## Intersection of top 20 city dest/origin traffic connection to NV

```{r}
library(dplyr)

# Join the two data frames to match the destinations with the origins
frequented_flights_combined <- most_frequented_destinations %>%
  # Rename Count column to Count_Dest for clarity
  rename(Count_Dest = Count) %>%
  select(DEST, Count_Dest) %>%
  # Perform the left join with most_frequented_origins
  left_join(
    most_frequented_origins %>%
      # Rename Count column to Count_Origin for clarity
      rename(Count_Origin = Count) %>%
      select(ORIGIN, Count_Origin),
    by = c("DEST" = "ORIGIN")
  )

frequented_flights_combined

```

```{r}
library(dplyr)

# Add a key to join on based on the row number after arranging by Count
most_frequented_destinations <- most_frequented_destinations %>%
  mutate(Join_Key = row_number())

most_frequented_origins <- most_frequented_origins %>%
  mutate(Join_Key = row_number())

# Join the two data frames on the Join_Key
frequented_flights_combined <- most_frequented_destinations %>%
  select(DEST, Count_Dest = Count, Join_Key) %>%
  left_join(most_frequented_origins %>%
              select(ORIGIN, Count_Origin = Count, Join_Key),
            by = "Join_Key") %>%
  select(-Join_Key) # remove the Join_Key as it's no longer needed

frequented_flights_combined

```

This type of flow constitutes in an example of 'sister cities' in term of air traffic.

```{r}
library(dplyr)

# Add a key to join on based on the row number after arranging by Count
most_frequented_destinations <- most_frequented_destinations %>%
  mutate(Join_Key = row_number())

most_frequented_origins <- most_frequented_origins %>%
  mutate(Join_Key = row_number())

# Join the two data frames on the Join_Key
frequented_flights_combined <- most_frequented_destinations %>%
  left_join(most_frequented_origins, by = "Join_Key")

frequented_flights_combined


```

# International or Domestic

```{r}
nv_flights_country_dest_analysis <- nv %>%
  # Filter for flights both originating in Nevada
  filter(ORIGIN_STATE_ABR == "NV") %>%
  # Categorize flights based on whether they have destination in the US or not
  mutate(Country_Category = if_else(DEST_COUNTRY == "US", "US", "Non-US")) %>%
  # Group by the new category
  group_by(Country_Category) %>%
  # Count the number of flights in each category
  summarise(Count = n(), .groups = 'drop') %>%
  # Optionally, calculate the percentage of total flights in each category
  mutate(Percentage = round((Count / sum(Count)) * 100, 2))

nv_flights_country_dest_analysis


```

```{r}
nv_flights_country_origin_analysis <- nv %>%
  # Filter for flights both originating in Nevada
  filter(DEST_STATE_ABR == "NV") %>%
  # Categorize flights based on whether they have destination in the US or not
  mutate(Country_Category = if_else(ORIGIN_COUNTRY == "US", "US", "Non-US")) %>%
  # Group by the new category
  group_by(Country_Category) %>%
  # Count the number of flights in each category
  summarise(Count = n(), .groups = 'drop') %>%
  # Optionally, calculate the percentage of total flights in each category
  mutate(Percentage = round((Count / sum(Count)) * 100, 2))

nv_flights_country_origin_analysis
```

We see the same symmetry holds for the number of flights coming/going from international origins/destinations holds as well.

## International Destinations

```{r}
# Create a new DataFrame with flights originating in Nevada and a label for international flights
nv_international_flights <- nv %>%
  # Filter for flights originating in Nevada
  filter(ORIGIN_STATE_ABR == "NV") %>%
  # Create a column that labels a flight as 'International' or 'Domestic'
  mutate(international = if_else(DEST_COUNTRY != "US", "International", "Domestic"))

# To find out the destination countries labeled as 'Non-US' from previous analysis
non_us_destinations <- nv_international_flights %>%
  filter(international == "International") %>%
  distinct(DEST_COUNTRY) %>%
  arrange(DEST_COUNTRY)

non_us_destinations

```

```{r}
colnames(nv_international_flights)
```

```{r}

# Summarize the number of flights and total passengers to each non-US destination
non_us_destinations_summary <- nv_international_flights %>%
  # Filter for international flights
  filter(international == "International") %>%
  # Group by destination country
  group_by(DEST_COUNTRY_NAME) %>%
  # Summarize the number of flights and total passengers
  summarise(
    Number_of_Flights = n(),
    Total_Passengers = sum(Sum_PASSENGERS, na.rm = TRUE),
    P2Fratio = Total_Passengers / Number_of_Flights,
    .groups = 'drop'
  ) %>%
  # Arrange by the number of flights to each destination for easier reading
  arrange(desc(Number_of_Flights))

non_us_destinations_summary

```

Looks like longer flights have a larger passenger per flight average. Notice how with 181 flights to S. Korea from Nevada we see a total of 460772 passengers moved. While with 187 flights to France we see 57199 passengers moved. That is almost a ninth of the passenger load with almost exactly the same number of flights.\
\
On average how far are both S. Korea and France from Nevada?

Flight path calculators estimate Seoul S. Korea to be roughly 6000 miles away, while we find Paris to be 5400 miles away. In this case we find distance isn't a determining factor in why such larger flights are used in connection with Nevada and S. Korea. Is this because of different airlines using different scales of planes.

Hmm adding a people to flight ratio shows we might need to investigate our summarized data further.

## Explore a specific airline/carrier/origin/destination combo

```{r}
library(dplyr)

# Create a subset of the nv dataset based on specific conditions
nv_sub1 <- nv %>%
  filter(CARRIER_NAME == "Southwest Airlines Co.",
         DEST == "LAS", 
         DEST_STATE_ABR == "NV", 
         ORIGIN == "RNO", 
         ORIGIN_STATE_ABR == "NV")

nv_sub2 <- nv %>%
  filter(CARRIER_NAME == "Southwest Airlines Co.",
         DEST == "RNO", 
         DEST_STATE_ABR == "NV", 
         ORIGIN == "LAS", 
         ORIGIN_STATE_ABR == "NV")


nv_sub3 <- nv %>%
  filter(AIRLINE_ID == 19393,
         # CARRIER_NAME == "Southwest Airlines Co.",
         DEST == "RNO",
         DEST_STATE_ABR == "NV",
         ORIGIN == "SMF",
         ORIGIN_STATE_ABR == "CA"
         )

nv_sub4 <- nv %>%
  filter(AIRLINE_ID == 19393,
         # CARRIER_NAME == "Southwest Airlines Co.",
         DEST == "SMF",
         DEST_STATE_ABR == "CA",
         ORIGIN == "RNO",
         ORIGIN_STATE_ABR == "NV"
         )
```

Yeah, it looks like the records are monthy passenger reports for the combination of (airline_id, origin, destination), if you reverse the origin destination with the same airline id then you will get different monthly numbers. So I'm not sure if the sum represents the outgoing or incoming passengers, but it does distinctly represent one of them.

So our previous tables was number of flights moving passengers but the number of months carriers reported moving passengers to the given countries. So let's fix that table

Num_of_monthly_carrier_records

### Num_of_monthly_carrier_records

```{r}
# Summarize the number of flights and total passengers to each non-US destination
non_us_destinations_summary <- nv_international_flights %>%
  # Filter for international flights
  filter(international == "International") %>%
  # Group by destination country
  group_by(DEST_COUNTRY_NAME) %>%
  # Summarize the number of flights and total passengers
  summarise(
    Num_of_monthly_carrier_records = n(),
    Total_Passengers = sum(Sum_PASSENGERS, na.rm = TRUE),
    P2Fratio = Total_Passengers / Num_of_monthly_carrier_records,
    .groups = 'drop'
  ) %>%
  # Arrange by the number of flights to each destination for easier reading
  arrange(desc(Num_of_monthly_carrier_records))

non_us_destinations_summary
```

So now we can reinterpret this table. We see that with 4007 total monthly records of airline carriers reporting passengers being flown to Canada for a total of 12.42 million people being flown over 20 years for about 3101 passengers to carrier-month report. We could probably build a better metric than that now that we know what better understand how the Sum_Passengers records are formed uniquely by (year, month, airline_id,origin,dest).\

```{r}
library(dplyr)

# Create a subset with unique tuples of (year, month, airline_id, origin, dest)
nv_unique_subset <- nv %>%
  distinct(YEAR, MONTH, AIRLINE_ID, ORIGIN, DEST, .keep_all = TRUE)

nv_unique_subset

```

This is proven quite easily by doing a subset on the tuple (year, month, airline_id,origin,destination) for `nv` in which we find they are the exact same dataset.

Knowing this, we can measure the monthly flow of every unique carrier in both the Las Vegas and Reno airports. We can like at the churn to see if some carriers send as many as they receive over time or if they act as a stepping stone to other terminals.

## Flow Table/Map

We can build a flow dataset by using an inbound/outbound label based upon if NV is the destination/origin. We make this choice because it is reasonable to believe a carrier would report the outgoing passengers corresponding to ticket sells, than to report the incoming passengers (which should not change midflight).

```{r}
# Create the nv_trafficflow dataset with an inbound/outbound label
nv_trafficflow <- nv %>%
  mutate(Flow = if_else(DEST_STATE_ABR == "NV", "Inbound",
                        if_else(ORIGIN_STATE_ABR == "NV", "Outbound", "Other")))

nv_trafficflow

```

```{r}
# Display the count of rows labeled as "Other" directly, and show them if there are any
if (any(nv_trafficflow$Flow == "Other")) {
  print(nv_trafficflow %>% filter(Flow == "Other"))
} else {
  cat("No rows labeled as 'Other'. Data is as expected.")
}

```

## Airlines with records spanning (2000,1) to (2019,12)

```{r}
# Generate all (year, month) combinations from 2000 to 2019
all_dates <- expand.grid(
  YEAR = 2000:2019,
  MONTH = 1:12
)

# Function to check if all date combinations are present for a given airline and flow type
is_complete_airline_flow <- function(airline_id, flow_type, df, all_dates) {
  airline_dates <- df %>% 
    filter(AIRLINE_ID == airline_id, Flow == flow_type) %>%
    select(YEAR, MONTH) %>%
    distinct() %>%
    arrange(YEAR, MONTH)
  
  # Convert to character to ensure easy comparison
  expected <- with(all_dates, paste(YEAR, MONTH))
  actual <- with(airline_dates, paste(YEAR, MONTH))
  
  # Check if all expected combinations are in the actual data
  all(expected %in% actual)
}

# Apply the function to each airline for inbound and outbound
complete_airlines_inbound <- Filter(function(id) is_complete_airline_flow(id, "Inbound", nv_trafficflow, all_dates), unique(nv_trafficflow$AIRLINE_ID))
complete_airlines_outbound <- Filter(function(id) is_complete_airline_flow(id, "Outbound", nv_trafficflow, all_dates), unique(nv_trafficflow$AIRLINE_ID))

# Get the corresponding carrier names for the complete airlines
complete_airline_names_inbound <- nv_trafficflow %>%
  filter(AIRLINE_ID %in% complete_airlines_inbound) %>%
  distinct(AIRLINE_ID, CARRIER_NAME) %>%
  arrange(AIRLINE_ID)

complete_airline_names_outbound <- nv_trafficflow %>%
  filter(AIRLINE_ID %in% complete_airlines_outbound) %>%
  distinct(AIRLINE_ID, CARRIER_NAME) %>%
  arrange(AIRLINE_ID)


```

```{r}
# Create indicators for inbound and outbound completeness
inbound_indicator <- complete_airline_names_inbound %>%
  mutate(Inbound = "Yes")

outbound_indicator <- complete_airline_names_outbound %>%
  mutate(Outbound = "Yes")

# Combine the two lists using a full join
combined_airline_completeness <- full_join(inbound_indicator, outbound_indicator, by = "AIRLINE_ID") %>%
  # Select and rename columns to make it clearer
  select(AIRLINE_ID, CARRIER_NAME = CARRIER_NAME.x, Inbound, Outbound) %>%
  # Replace NA with "No" to indicate not complete for that traffic type
  mutate(Inbound = replace_na(Inbound, "No"),
         Outbound = replace_na(Outbound, "No"))

# Print the combined table
print(combined_airline_completeness)


```

```{r}
library(dplyr)
library(tidyr)

# Create a simplified version of both inbound and outbound dataframes with only airline_id and carrier_name
inbound_indicator <- complete_airline_names_inbound %>%
  rename(Inbound = CARRIER_NAME)

outbound_indicator <- complete_airline_names_outbound %>%
  rename(Outbound = CARRIER_NAME)

# Combine the two indicators using a full join to ensure all airline IDs are included
combined_airline_completeness <- full_join(inbound_indicator, outbound_indicator, by = "AIRLINE_ID") %>%
  # Replace NA with empty string to indicate not complete for that traffic type
  mutate(Inbound = replace_na(Inbound, ""),
         Outbound = replace_na(Outbound, ""))

# Print the combined table
print(combined_airline_completeness)


```

Looks like we have 7 airlines with complete inbound/outbound data and they all match.

## Combining Inbound/Outbound Flows

We can downsample the flows averaging, summing, differencing (airline_id, year, month,origin,dest) values along being inbound or outbound.

```{r}
aggregated_flows <- nv_trafficflow %>%
  group_by(AIRLINE_ID, YEAR, MONTH, ORIGIN, DEST, Flow) %>%
  arrange(AIRLINE_ID, YEAR, MONTH)

```

```{r}

# Normalize the origin and destination to create undirected pairs
# nv_trafficflow <- nv_trafficflow %>%
#   mutate(
#     Route = apply(select(., ORIGIN, DEST), 1, function(x) paste(sort(x), collapse = "-"))
#   )

nv_trafficflow <- nv_trafficflow %>%
  mutate(
    Route = apply(select(., ORIGIN, DEST), 1, function(x) {
      # Sort the cities, but ensure Nevada cities "LAS" or "RNO" are always first if present
      if ("LAS" %in% x) {
        route <- c("LAS", setdiff(x, "LAS"))
      } else if ("RNO" %in% x) {
        route <- c("RNO", setdiff(x, "RNO"))
      } else {
        route <- sort(x)
      }
      paste(route, collapse = "-")
    })
  )



# Now aggregate by airline_id, year, month, and the normalized route
total_passengers_by_route <- nv_trafficflow %>%
  group_by(AIRLINE_ID, YEAR, MONTH, Route) %>%
  summarise(
    Total_Passengers = sum(Sum_PASSENGERS, na.rm = TRUE),
    .groups = 'drop'
  )


```

```{r}

# Now aggregate by airline_id, year, month, and the normalized route
total_passengers_by_route <- nv_trafficflow %>%
  group_by(AIRLINE_ID, YEAR, MONTH, Route) %>%
  summarise(
    Total_Passengers = sum(Sum_PASSENGERS, na.rm = TRUE),
    # Calculate the difference
    Difference = diff(Sum_PASSENGERS),
    # Calculate the ratio, safely handling division by zero
    Ratio = if_else(Sum_PASSENGERS[2] != 0, Sum_PASSENGERS[1] / Sum_PASSENGERS[2], NA_real_),
    .groups = 'drop'
  )

# List of AIRLINE_ID values from your provided image
nv_complete_airlines <- c(19393, 19690, 19790, 19805, 19930, 19977, 20436)

# Filter the total_passengers_by_route for these specific airline IDs
nv_complete_airlines <- total_passengers_by_route %>%
  filter(AIRLINE_ID %in% nv_complete_airlines)

```

## Create time-series pivot table for complete NV airlines

```{r}
pivot_airlines_sum <- nv_complete_airlines %>%
  mutate(datetime = as.Date(paste(YEAR, MONTH, "01", sep = "-"))) %>%
  group_by(AIRLINE_ID, datetime) %>%
  summarize(Total_Passengers = sum(Total_Passengers), .groups = "drop_last") %>%
  pivot_wider(names_from = AIRLINE_ID, values_from = Total_Passengers)

head(pivot_airlines_sum)
```

```{r}
colnames(combined_airline_completeness)
```

```{r}
library(ggplot2)
library(tidyr)

# Convert from wide to long format for ggplot
pivot_airlines_long <- pivot_airlines_sum %>%
  pivot_longer(
    cols = -datetime, 
    names_to = "AIRLINE_ID", 
    values_to = "Total_Passengers"
  )

# Plotting the time series data
ggplot(pivot_airlines_long, aes(x = datetime, y = Total_Passengers, color = AIRLINE_ID, group = AIRLINE_ID)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Date", y = "Total Passengers", title = "Airline Passenger Traffic Over Time") +
  theme(legend.title = element_blank())  # Optional: remove legend title

```

```{r}
# Create a dictionary from combined_airline_completeness data frame
airline_id_dict <- combined_airline_completeness %>%
  select(AIRLINE_ID, Inbound) %>%
  distinct() %>%
  # Create a named vector (dictionary)
  { setNames(.$Inbound, .$AIRLINE_ID) }

# Add airline names to the pivot_airlines_long data frame for ggplot
pivot_airlines_long <- pivot_airlines_sum %>%
  pivot_longer(
    cols = -datetime, 
    names_to = "AIRLINE_ID", 
    values_to = "Total_Passengers"
  ) %>%
  # Use the dictionary to get airline names
  mutate(AIRLINE_NAME = airline_id_dict[as.character(AIRLINE_ID)])

# Plotting with labels from the dictionary
ggplot(pivot_airlines_long, aes(x = datetime, y = Total_Passengers, group = AIRLINE_ID)) +
  geom_line(aes(color = AIRLINE_NAME)) +  # Use airline names for the line colors
  theme_minimal() +
  labs(x = "Date", y = "Total Passengers", title = "Airline Passenger Traffic Over Time", color = "Airline") +
  theme(legend.title = element_blank())  # Optional: remove legend title if desired


```

### Combining Airlines

Suppose instead of looking at one or two major airlines we create a metric to account for all airlines reporting in a single month from a single location.

# Forecasting SW-Airlines Total Monthly Passenger Traffic

We never subset on Las Vegas and Reno given by `LAS` and `RNO`, but we will find that they are the only airports which had airlines that reported consistent monthly records over 20 years. Our table `pivot_airlines_xts` was contructed from `nv_complete_airlines`, so we can check to make sure very `Route` record begins with either `LAS-` or `RNO-`

```{r}

# Check if all Route records begin with "LAS-" or "RNO-"
all_routes_start_correctly <- all(
  grepl("^LAS-|^RNO-", nv_complete_airlines$Route)
)

# Print the result directly
print(all_routes_start_correctly)

```

So our first model will look at SouthWest Airlines total monthly passenger transport (this includes inbound and outbound passenger traffic). This model give us an idea of how much total passenger traffic SouthWest Airlines should prepare for in a given year and around what times. Such a model could help SW Airlines schedule more thorough safety inspections of aircrafts in the main hubs Las Vegas and Reno Nevada.

We will subset `pivot_airlines_sum` for southwest airlines given by `AIRLINE_ID = 19393`.

```{r}
# Extract the column for Southwest Airlines, ensuring it's a vector (univariate series)
southwest_series <- pivot_airlines_sum[["19393"]]

# Since the datetime column is already in a proper date format, extract year and month directly
start_year <- as.numeric(format(pivot_airlines_sum$datetime[1], "%Y"))
start_month <- as.numeric(format(pivot_airlines_sum$datetime[1], "%m"))

# Create a ts object using the extracted series, specifying the start time and frequency
southwest_ts <- ts(southwest_series, start = 2000, frequency = 12)

# Verification: Check if it's now a univariate time series
print(is.ts(southwest_ts))  # Should return TRUE
print(length(dim(southwest_ts)))  # Should return NULL for a univariate ts object


# Check the frequency of the time series
ts_frequency <- frequency(southwest_ts)

# Print the frequency
print(ts_frequency)
class(southwest_ts)
```

### Moving Averages for SW Airlines

```{r}
library(forecast)
autoplot(southwest_ts) +
  autolayer(ma(southwest_ts, order = 3), series = "3-MA", na.rm = TRUE) +
  autolayer(ma(southwest_ts, order = 6), series = "6-MA", na.rm = TRUE) +
  autolayer(ma(southwest_ts, order = 12), series = "12-MA", na.rm = TRUE)
```

```{r}
library(forecast)  # For ma() and autoplot()
library(ggplot2)   # For additional plotting capabilities

# Create the plot with thicker lines
plot <- autoplot(southwest_ts, size = 0.75) +
  autolayer(ma(southwest_ts, order = 3), series = "3-MA", na.rm = TRUE, size = 1) +
  autolayer(ma(southwest_ts, order = 4), series = "4-MA", na.rm = TRUE, size = 1) +
  autolayer(ma(southwest_ts, order = 6), series = "6-MA", na.rm = TRUE, size = 1) +
  autolayer(ma(southwest_ts, order = 12), series = "12-MA", na.rm = TRUE, size = 1) +
  labs(title = "Southwest Airlines Passenger Traffic with Moving Averages",
       x = "Time", y = "Total Passengers") +
  theme_minimal() +
  theme(text = element_text(size = 14))  # Increase text size for better readability

# Display the plot in the R plotting window
print(plot)

# Save the plot with high resolution
ggsave("southwest_ts_plot.png", plot = plot, width = 16, height = 6, dpi =  1000)

```

## Decomps

### X11 Decomp Monthly 

```{r}
SW_x11 <- seas(southwest_ts, x11 = "")
autoplot(SW_x11) +
  ggtitle("decomposition by X11")
```

### SEATs Decomp

```{r}
SW_seats <- seas(southwest_ts)
autoplot(SW_seats) +
  ggtitle("Decomposition using SEATS")
```

### STL Decomp

```{r}
# Ensure it's a univariate time series
print(is.ts(southwest_ts))  # Should return TRUE
print(length(dim(southwest_ts)))  # Should return NULL or 1

```

```{r}
SW_stl <- stl(southwest_ts, s.window = 13, t.window = 5)
autoplot(SW_stl) +
  ggtitle("Decomposition by STL")
```

## Season Plots

```{r}
ggseasonplot(southwest_ts)
```

## Test Train Split

```{r}
train_ratio <- 0.9
N <- length(southwest_ts)
T <- floor(N*train_ratio)
S <- N - T
SW_tr <- head(southwest_ts, T)
SW_ts <- tail(southwest_ts, S)
```

## Basic Forecast

### Naive

```{r}
SW_tr_n <- naive(SW_tr, h = S)
SW_tr_n
```

### Drift

```{r}
SW_tr_d <- rwf(SW_tr, h = S, drift = TRUE)
SW_tr_d
```

### Season Naive

```{r}
SW_tr_sn <- snaive(SW_tr, h = S)
SW_tr_sn
```

### Holt-Winters

```{r}
SW_tr_hw <- hw(SW_tr, h = S)
SW_tr_hw
```

### ETS Models

```{r}
# damped trend
SW_tr_ets_damp <- ets(SW_tr, model = "MAM", damped = TRUE)
SW_etsf_damp <- forecast(SW_tr_ets_damp, h = S)

summary(SW_etsf_damp)
```

```{r}
checkresiduals(SW_etsf_damp)
```

ETSF Damp MAM model is the first to have residuals which borderline resemble whitenoise with a p-value of 0.05

```{r}
SW_tr_best_ets <- ets(SW_tr, model = "ZZZ", ic = "bic")
SW_best_estf <- forecast(SW_tr_best_ets, h = S)
summary(SW_tr_best_ets)
checkresiduals(SW_best_estf)
accuracy(SW_best_estf, SW_ts)
```

## Model Forecast Plots

```{r}
autoplot(SW_tr) + 
  autolayer(SW_tr_n, PI = FALSE, series = "naive") +
  autolayer(SW_tr_sn, PI = FALSE, series = "s naive") +
  autolayer(SW_tr_d, PI = FALSE, series = "drift") +
  autolayer(SW_tr_hw, PI = FALSE, series = "holt-winters") +
  autolayer(SW_etsf_damp, PI = FALSE, series = "MAM etsf damp") +
  autolayer(SW_best_estf, PI = FALSE, series = "etsf best") +
  autolayer(SW_ts, series = "testing")
```

### Exponential Smoothing (notebook 13ES)

```{r}
SW_stl_tr <- stl(SW_tr, t.window = 13, s.window = "periodic")
autoplot(SW_stl_tr)
```

We apply simple exponential smoothing on the seasonally adjusted data

```{r}
SW_stl_tr_seasa <- seasadj(SW_stl_tr)

SW_stl_tr_ses <- ses(SW_stl_tr_seasa, h = S)

autoplot(SW_stl_tr_seasa) +
  autolayer(SW_stl_tr_ses)
```

## Residuals

```{r}
checkresiduals(SW_tr_n)
checkresiduals(SW_tr_sn)
checkresiduals(SW_tr_d)
checkresiduals(SW_tr_hw)
checkresiduals(SW_stl_tr_ses)
```

We see that our p-value is very small and thus the residuals are not a white-nose series, and conclude that the naive, s naive, and drift forecast method are poor fits for monthly total passenger forecasting for SW airlines. Simple exponential smoothing performs a little better but the residuals still do not resemble white noise.\
\
Holt-Winters with the highest p-value so far, but still not close to 0.05

## Accuracy

```{r}
accuracy(SW_tr_n, SW_ts)
accuracy(SW_tr_sn, SW_ts)
accuracy(SW_tr_d, SW_ts)
accuracy(SW_tr_hw, SW_ts)
accuracy(SW_etsf_damp, SW_ts)
accuracy(SW_stl_tr_ses, SW_ts)
```

## Cross Validation `tsCV()`

```{r}
rmse_table <- rbind(RMSE = rep(NA, 8))
colnames(rmse_table) <- c("naive", "naive-BC", "drift", "drift-BC", "mean", "mean-BC", "snaive", "snaive-BC")
```

```{r}
# naive
error_cv <- tsCV(SW_tr, naive, h = 1)
rmse_table["RMSE", "naive"] <- sqrt(mean(error_cv^2, na.rm = TRUE))

# bc naive
nb <- function(y, h){
  la <- BoxCox.lambda(y)
  return (naive(y, h = h, lambda = la))
}

error_cv <- tsCV(SW_tr, nb, h = 1)
rmse_table["RMSE", "naive-BC"] <- sqrt(mean(error_cv^2, na.rm = TRUE))

# drift
drift <- function(y, h){
  return (rwf(y, h = h, drift = TRUE))
}

error_cv <- tsCV(SW_tr, drift, h = 1)
rmse_table["RMSE", "drift"] <- sqrt(mean(error_cv^2, na.rm = TRUE))

# drift BC
driftBC <- function(y, h){
  la <- BoxCox.lambda(y)
  return (rwf(y, h = h, lambda = la, drift = TRUE))
}

error_cv <- tsCV(SW_tr, driftBC, h = 1)
rmse_table["RMSE", "drift-BC"] <- sqrt(mean(error_cv^2, na.rm = TRUE))

# mean forecast
error_cv <- tsCV(SW_tr, meanf, h = 1)
rmse_table["RMSE", "mean"] <- sqrt(mean(error_cv^2, na.rm = TRUE))

# mean BC
meanfBC <- function(y, h){
  la <- BoxCox.lambda(y)
  return (meanf(y, h = h, lambda = la))
}

error_cv <- tsCV(SW_tr, meanfBC, h = 1)
rmse_table["RMSE", "mean-BC"] <- sqrt(mean(error_cv^2, na.rm = TRUE))

# naive
error_cv <- tsCV(SW_tr, snaive, h = 1)
rmse_table["RMSE", "snaive"] <- sqrt(mean(error_cv^2, na.rm = TRUE))

# bc naive
snb <- function(y, h){
  la <- BoxCox.lambda(y)
  return (snaive(y, h = h, lambda = la))
}

error_cv <- tsCV(SW_tr, snb, h = 1)
rmse_table["RMSE", "snaive-BC"] <- sqrt(mean(error_cv^2, na.rm = TRUE))
```

```{r}
rmse_table
```

## Is SW airlines time-series stationary?

```{r}
adf.test(southwest_ts)
```

p-value = 0.1312 \> 0.05, conclusion: not stationary.

```{r}
summary(ur.kpss(southwest_ts))
```

3.7531 is greater than 0.463, so p-value is less than 0.5%, so not stationary.

## Seasonal Difference

```{r}
nsdiffs(southwest_ts)
```

```{r}
sw_diff12 <- southwest_ts %>%
  diff(., lag = 12) %>%
  diff
autoplot(sw_diff12)
ggseasonplot(sw_diff12)
```

Lets check if the seasonally differenced series is stationary

```{r}
adf.test(sw_diff12)
```

The ADF test gives us a pvalue less than 0.05, so our conclusion is stationary.

```{r}
summary(ur.kpss(sw_diff12))
```

Our t-test value of 0.667 is substantial smaller than the non-seasonally differenced series, but is greater than 0.463 and we conclude the series is not stationary.

### Our Stationary SW airlines series

When we apply the seasonal differencing and then first differencing we find both the ADF and KPSS test conclude a stationary series as the result.

```{r}
ggtsdisplay(sw_diff12)
```

### ARIMA & Stationary Time Series

```{r}
# arma03 <- <- arima.sim(model = list(order = c(0, 0, 3)))
sw_sj_tr <- seasadj(stl(SW_tr, s.window = "periodic", t.window = 13))
summary(ur.kpss(sw_sj_tr))
```

```{r}
summary(ur.kpss(diff(sw_sj_tr)))
```

```{r}
ggtsdisplay(diff(sw_sj_tr))
```

initial fit: AR(2)

```{r}
sw_sj_auto <- auto.arima(sw_sj_tr, d=1, D = 0, max.p = 2, max.q = 2, seasonal = FALSE)
sw_sj_auto
```

```{r}
checkresiduals(sw_sj_auto)
```

```{r}
autoplot(sw_sj_tr) + 
  autolayer(forecast(sw_sj_auto, h = S))
```

We treated our problem as if it wasn't seasonal when our KPSS let us know this might not be best. Now let's try a season arima

## Seasonal ARIMA

```{r}
sw_sdiff <- SW_tr %>%
  diff(., lag = 12) %>%
  diff
autoplot(sw_sdiff)
ggseasonplot(sw_sdiff)
```

```{r}
ggtsdisplay(sw_sdiff, lag.max = 50)
```

Non-seasonal part: for ACF, there is 1 spike, for PACF 3 spikes.

Seasonal part (looking at lags who are multiple of 12)

-   For `ACF`: 1 spike

-   For `PACF`: 2, maybe 3, spikes.

```{r}
fit1 <- Arima(SW_tr, lambda = 0, order = c(1,0,0), seasonal = c(0,1,2), biasadj = TRUE) # AIC = -696.23
fit2 <- Arima(SW_tr, lambda = 0, order = c(2,0,0), seasonal = c(0,1,2), biasadj = TRUE) # AIC = -725.2
fit3 <- Arima(SW_tr, lambda = 0, order = c(3,0,0), seasonal = c(0,1,2), biasadj = TRUE) # AIC = -727.02
fit4 <- Arima(SW_tr, lambda = 0, order = c(3,1,0), seasonal = c(0,1,3), biasadj = TRUE) # AIC = -731.85
fit1
fit2
fit3
fit4
```

```{r}
checkresiduals(fit4)
```

```{r}
# ARIMA(3,1,0)(0,1,3) (p,d,q)(P,D,Q)
sw_tr_autoarima_s <- auto.arima(SW_tr, d = 1, D = 1, max.p = 3, max.q = 3, max.P = 3, max.Q = 3,
                                lambda = 0, ic = "aicc", stepwise = FALSE)
sw_tr_autoarima_s
```

## ARIMA(0,1,1)(0,1,3)[12]

```{r}
autoplot(SW_tr) +
  autolayer(forecast(sw_tr_autoarima_s, h = S), PI = FALSE, series = "seasonal autoarima") +
  autolayer(forecast(fit4, h = S), PI = FALSE, series = "fit4") +
  autolayer(SW_ts)
```

```{r}
checkresiduals(sw_tr_autoarima_s)
```

## Fourier Analysis

```{r}
head(fourier(SW_tr, K = 4))
```

```{r}
Tr <- length(SW_tr)
t <- 1:Tr
sq_t <- sqrt(t)
la <- BoxCox.lambda(SW_tr)
print(la)
la_t <- t^(la)
log_t <- log(t)
sw_fourier_tr <- tslm(SW_tr ~ la_t + fourier(SW_tr, K = 4))
summary(sw_fourier_tr)
```

```{r}
autoplot(SW_tr) +
  autolayer(fitted(sw_fourier_tr))
```

```{r}
library(forecast)

# Generating Fourier terms correctly for the specified forecast horizon
future_fourier <- fourier(SW_tr, K = 6, h = 24)  # Ensures it generates terms for 24 future steps

# Correct future_la_t to match the forecast horizon
future_t <- (Tr+1):(Tr+24)  # Confirm this results in 24 periods
future_la_t <- future_t^(la)  # Compute la_t for the future periods, ensuring it's length 24

# Now forecast using the tslm model and the newdata frame with correctly sized inputs
#sw_forecast <- forecast(sw_fourier_tr, newdata = data.frame(la_t = future_la_t, fourier = I(future_fourier)))

future_data <- data.frame(la_t = future_la_t)

future_data <- cbind(future_data, future_fourier)  # Directly bind the
# Now forecast using the tslm model with the correctly structured newdata
sw_forecast <- forecast(sw_fourier_tr, newdata = future_data)

# Plotting the original series with forecasts
autoplot(SW_tr) +
  autolayer(sw_forecast$mean, series = "Forecast")+
  autolayer(SW_ts, series = "testset")


```

```{r}
accuracy(sw_forecast, SW_ts)
```
